# Advanced RAG Architecture

```mermaid
graph TB
    subgraph "INGESTION PHASE (pro_implementation/ingest.py)"
        A[ğŸ“„ Load Documents<br/>Markdown Files] --> B[ğŸ¤– LLM Chunking<br/>GPT-4.1-nano intelligently splits<br/>25% overlap, semantic boundaries]
        B --> C[âœ¨ Enhance Chunks<br/>Add Headline + Summary + Original Text<br/>for each chunk]
        C --> D[ğŸ§® Generate Embeddings<br/>OpenAI text-embedding-3-large]
        D --> E[ğŸ’¾ Store in ChromaDB<br/>preprocessed_db/]
        
        B -.-> F[âš™ï¸ Parallel Processing<br/>3 workers<br/>+ retry logic]
    end
    
    subgraph "QUERY PHASE (pro_implementation/answer.py)"
        G[ğŸ‘¤ User Question] --> H[ğŸ”„ Query Rewriting<br/>LLM refines question<br/>for better retrieval]
        
        H --> I1[ğŸ” Vector Search #1<br/>Original Question<br/>Top K=20]
        H --> I2[ğŸ” Vector Search #2<br/>Rewritten Question<br/>Top K=20]
        
        E -.->|Retrieve| I1
        E -.->|Retrieve| I2
        
        I1 --> J[ğŸ”— Merge & Dedupe<br/>Combine results]
        I2 --> J
        
        J --> K[ğŸ¯ LLM Reranking<br/>Semantic relevance scoring<br/>by GPT-4.1-nano]
        
        K --> L[ğŸ“Š Top K=10<br/>Best ranked chunks]
        
        L --> M[ğŸ“‹ Format Context<br/>Enhanced chunks with sources]
        
        M --> N[ğŸ¤– LLM Generation<br/>GPT-4.1-nano or Groq<br/>With optimized context]
        
        N --> O[âœ… Answer + Sources]
    end
    
    style A fill:#e3f2fd
    style C fill:#f3e5f5
    style E fill:#fff3e0
    style G fill:#e8f5e9
    style H fill:#fff9c4
    style K fill:#fff9c4
    style N fill:#fff9c4
    style O fill:#c8e6c9
    style F fill:#ffebee
```

## Flow Characteristics

### Ingestion
- **LLM-powered chunking**: Semantic understanding of document structure
- **Enhanced metadata**: Headlines optimize for query matching
- **Parallel processing**: 3x faster with multiprocessing
- **Retry logic**: Handles rate limits and transient errors

### Query  
- **Query rewriting**: LLM extracts key terms and focuses the search
- **Dual retrieval**: Searches with both original and rewritten queries
- **Merge & dedupe**: Combines results without duplicates
- **LLM reranking**: Semantic relevance beats pure cosine similarity
- **Top-K selection**: Only best 10 chunks go to final generation

### Performance
- ğŸ¢ **Speed**: Slower (3-5s per query) - 3 extra LLM calls
- ğŸ’° **Cost**: Higher ($0.01-0.02 per query)
- ğŸ¯ **Accuracy**: 85-95% retrieval quality (+15-25% improvement)

## Key Improvements Over Basic RAG

| Component | Basic | Advanced | Benefit |
|-----------|-------|----------|---------|
| **Chunking** | Character-based | LLM semantic | Better chunk quality |
| **Metadata** | None | Headline + Summary | Improved matching |
| **Retrieval** | Single query | Dual query | Catches missed docs |
| **Ranking** | Cosine similarity | LLM reranking | Better relevance |
| **Processing** | Sequential | Parallel | Faster ingestion |
